{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mushroom Pokédex - The Mokédex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The aim of this project is to build a deep learning algorithm that can predict a species of mushroom from a photo, returning the species, common name, and whether it is edible, poisonous, or psychedelic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this project you need to have the correct packages installed. You may install them however you like based off of the import modules that appear in this notebook, but if you have retrieved this notebook from my github repo, you will notice a requirements.txt file which lists all relevant packages used in my projects based on fastai.\n",
    "Here is a link to the relevant repo folder: https://github.com/alexmarsian/Personal-Projects/tree/master/FastAI/Week1\n",
    "\n",
    "Yes it would be more efficient to create a new virtual environment for each separate project, but I suspect I will be returning to the same modules over and over again.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the jupyter notebook environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to webscrape the data on various mushrooms species. To do this I am going to use selenium webdriver to browse through relevant websites on google chrome, beautifulsoup to parse data from the html of a webpage, and the fastai datablock to store our information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First configure the selenium webdriver to use google chrome. You need to download the relevant chromedriver exe for your OS from here: https://sites.google.com/a/chromium.org/chromedriver/. I saved the chromedriver exe in the same dir as this notebook. So initialising the selenium webdriver to use the chrome exe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "path = os.getcwd() + '/chromedriver'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to gather Mushroom data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to start scraping data on mushrooms using Wikipedia. \n",
    "I will first scrape all the data on edible mushrooms which is less well organised, and then retrieve data on poisonous, deadly, and psychedelic mushrooms. \n",
    "The pages to be used for scraping are:\n",
    "* Edible: https://en.wikipedia.org/wiki/Edible_mushroom\n",
    "* Poisonous: https://en.wikipedia.org/wiki/List_of_poisonous_fungus_species\n",
    "* Deadly: https://en.wikipedia.org/wiki/List_of_deadly_fungus_species\n",
    "* Psychidelic: There are only 4, so we will retrieve their indivudal pages:\n",
    "    * Psilocybe Azurescens: https://en.wikipedia.org/wiki/Psilocybe_azurescens\n",
    "    * Psilocybe cubensis: https://en.wikipedia.org/wiki/Psilocybe_cubensis\n",
    "    * Psilocybe cyanescens: https://en.wikipedia.org/wiki/Psilocybe_cyanescens\n",
    "    * Psilocybe semilanceata: https://en.wikipedia.org/wiki/Psilocybe_semilanceata\n",
    "\n",
    "Given the variety of webpages to be used here we will not be able to write one script to scrape everything. Unfortunately there is no online encylopedia of mushrooms that contains all the information for every type of mushroom - that would allow this process to be automated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before scraping any data, we need to consider what information we want to collect, and how we should store it. \n",
    "I am going to create a mushroom class that will store:\n",
    "* Species (latin name)\n",
    "* Common Name\n",
    "* Type: Edble, Poisonous, Deadly, or Psychidelic\n",
    "* Info: A short description of the mushroom\n",
    "* Similar: Known look alikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mushroom:\n",
    "    \"\"\"Each mushroom contains a species, common name, frequency, and edibility\"\"\"\n",
    "    def __init__(self, species, name, mushroom_type, info, similar):\n",
    "        self.spec = species\n",
    "        self.name = name\n",
    "        self.type = mushroom_type\n",
    "        self.info = info\n",
    "        self.simi = similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to consider where that information may be retrieved from, and if there are any aspects of the data collection that will be uniform across all mushroom types i.e. can be contained within a single function\n",
    "\n",
    "So for all the edible mushrooms, the wikipedia page has a similar format of \"Species Name\" - Common Name or Info.\n",
    "\n",
    "<img src='edible-mushroom-eg.jpg'>\n",
    "\n",
    "Whilst the pages for poisonous and deadly mushrooms share almost exactly the same tabular format with Species name, common name, active agent, distribution, and similar edible species. \n",
    "\n",
    "<img src='toxic-mushroom-eg.jpg'>\n",
    "\n",
    "Notably there is not really any description on either site, but we can attain a description by going to the species page of a particular mushroom on wikipedia and going to the description field. The following is the wikipedia description for Gyromitra_esculenta: \n",
    "\n",
    "<img src='mushroom-info-eg.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Required Tasks for Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can gather species name, common name, and type for edible mushrooms from a single wikipedia page\n",
    "* We can gather species name, common name, type, and look alikes for both poisonous and deadly mushrooms using the same method as the data is organised identically on wikipedia\n",
    "* Once we have all the species names we want, we can use the same method to gather a description of every mushroom species based on the Wikipedia description\n",
    "* We can manually add the psychedelic species as there are only 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Data on Edible Mushrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the page containing edible mushrooms and retreive the html content of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    driver = webdriver.Chrome(executable_path=path)\n",
    "    driver.get(url)\n",
    "    html_content = driver.page_source\n",
    "    return html_content\n",
    "html_content = get_html(\"https://en.wikipedia.org/wiki/Edible_mushroom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_only = SoupStrainer(attrs='info')\n",
    "soup = BeautifulSoup(html_content, \"html.parser\", parse_only=parse_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to define a function that will go through each string in the parsed info divs, strip the brackets from the common name, split the frequency and edibility into two strings, and store the information into the relevant object instance in the Mushroom class, returning a list of mushroom objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup Object + List -> ListOfMushroom\n",
    "# Creates a list of mushroom objects given the parsed html content\n",
    "def get_mushies(soup, list_of_mushies):\n",
    "    all_strings = soup.stripped_strings\n",
    "    for string in all_strings:\n",
    "        species = string\n",
    "        common_name = next(all_strings)\n",
    "        common_name = common_name.replace(')', '').replace('(', '')\n",
    "        freq_and_edib = next(all_strings).split(',')\n",
    "        frequency = freq_and_edib[0].strip()\n",
    "        edibility = freq_and_edib[1].strip()\n",
    "        list_of_mushies.append(Mushroom(species, common_name, frequency, edibility))\n",
    "    return list_of_mushies\n",
    "\n",
    "mushies = get_mushies(soup, [])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
