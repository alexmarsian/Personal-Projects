{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mushroom Pokédex - The Mokédex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The aim of this project is to build a deep learning algorithm that can predict a species of mushroom from a photo, returning the species, common name, and whether it is edible, poisonous, or psychedelic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this project you need to have the correct packages installed. You may install them however you like based off of the import modules that appear in this notebook, but if you have retrieved this notebook from my github repo, you will notice a requirements.txt file which lists all relevant packages used in my projects based on fastai.\n",
    "Here is a link to the relevant repo folder: https://github.com/alexmarsian/Personal-Projects/tree/master/FastAI/Week1\n",
    "\n",
    "Yes it would be more efficient to create a new virtual environment for each separate project, but I suspect I will be returning to the same modules over and over again..\n",
    "\n",
    "Note as well this is not built out to its full capacity, this project was meant for me to learn about building a deep learning model using the fastai library. I have thought about how to create a more complete Mokédex, I have archived that for the time being in the mokédex-side-project directory in the fastai folder on my repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Collecting image URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this document we will have generated folders containing CSV files of image URLs and a list of all classes (labels). Downloading images and utilisation of the fastAI module will continue in another document 'week1-cnn'. \n",
    "\n",
    "I had to split this project into two documents because of difficulty uploading the chromedriver exe to paperspace gradient (where I have a GPU linked notebook). Thus in part 1 I will scrape the data, and in part 2 I will upload the CSV files to paperspace, download the images using fastAI, and train the CNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the jupyter notebook environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to webscrape the data on various mushrooms species. To do this I am going to use selenium webdriver to browse through relevant websites on google chrome, beautifulsoup to parse data from the html of a webpage, and the fastai datablock to store our information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First configure the selenium webdriver to use google chrome. You need to download the relevant chromedriver exe for your OS from here: https://sites.google.com/a/chromium.org/chromedriver/. I saved the chromedriver exe in the same dir as this notebook. So initialising the selenium webdriver to use the chrome exe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "path = os.getcwd() + '/chromedriver'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Mushroom Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to start scraping data on mushrooms using this website: http://www.foragingguide.com/\n",
    "I will first scrape all the data on edible mushrooms, and then retrieve data on poisonous mushrooms. We should be able to use the same scripts to scrape each set of data. \n",
    "\n",
    "For each mushroom we want to store:\n",
    "* Species (Latin Name)\n",
    "* Common Name\n",
    "* Type: Edible, Poisonous, Psychedelic\n",
    "* Frequency\n",
    "\n",
    "So here is a mushroom class for storing this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mushroom:\n",
    "    \"\"\"Each mushroom contains a species, common name, frequency, and edibility\"\"\"\n",
    "    def __init__(self, species, name, mushroom_type, frequency):\n",
    "        self.spec = species\n",
    "        self.name = name\n",
    "        self.type = mushroom_type\n",
    "        self.freq = frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the page containing edible mushrooms, sorted A-Z by latin name, retreive the html content of the page, and parse the html using BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    driver = webdriver.Chrome(executable_path=path)\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    return html\n",
    "edible_html = get_html(\"http://www.foragingguide.com/mushrooms/edible_by_latin_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parse a website's html we need to understand the structure of the html. To get an idea for this, I opened the webpage http://www.foragingguide.com/mushrooms/edible_by_latin_name, right clicked and chose 'inspect'. Looking at how the information for the first mushroom 'Agaricus augustus' is stored in the html, we see this structure in the inspect column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mushroom-html-example.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all the information we want for each individual mushroom is stored in strings within a div class=info. Each div class=info is within a div class=list_div. There are a number of ways to retreive the data we want, but I am going to try use the most memory efficient method. \n",
    "* We will use SoupStrainer to parse only div class=\"info\" elements, and then use the .stripped_strings method from BeautifulSoup to create a generator that returns descendant strings (whitespace stripped). \n",
    "* For each div class=info, we expect 3 strings to be returned.  \n",
    "    1. The latin name (species)\n",
    "    2. The common name, from which we will strip the opening '(' and closing ')'\n",
    "    3. The frequency and edibility in the format 'frequency, edibility' which we can split into two strings 'frequency' and 'edibility'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(edible_html, \"html.parser\", parse_only=SoupStrainer(attrs='info'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to define a function that will go through each string in the parsed info divs, strip the brackets from the common name, split the frequency and edibility into two strings, and store the information into the relevant object instance in the Mushroom class, returning a list of mushroom objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup Object + List -> ListOfMushroom\n",
    "# Creates a list of mushroom objects given the parsed html content\n",
    "def get_mushies(soup, list_of_mushies):\n",
    "    all_strings = soup.stripped_strings\n",
    "    for string in all_strings:\n",
    "        species = string\n",
    "        name = next(all_strings)\n",
    "        name = name.replace(')', '').replace('(', '')\n",
    "        freq_and_type = next(all_strings).split(',')\n",
    "        frequency = freq_and_type[0].strip().capitalize()\n",
    "        mushroom_type = freq_and_type[1].strip().capitalize()\n",
    "        list_of_mushies.append(Mushroom(species, name, mushroom_type, frequency))\n",
    "    return list_of_mushies\n",
    "\n",
    "mushies = get_mushies(soup, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see that is has worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species: Agaricus augustus\n",
      "Common name: The Prince\n",
      "Type: Edible good\n",
      "Frequency: Occasional\n"
     ]
    }
   ],
   "source": [
    "for m in mushies:\n",
    "    if m.spec == \"Agaricus augustus\":\n",
    "        print(\"Species: \" + m.spec)\n",
    "        print(\"Common name: \" + m.name)\n",
    "        print(\"Type: \" + m.type.capitalize())\n",
    "        print(\"Frequency: \" + m.freq.capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this process can be repeated for the poisonous mushrooms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get html content for the poisonous mushrooms\n",
    "poisonous_html = get_html(\"http://www.foragingguide.com/mushrooms/poisonous_by_latin_name\")\n",
    "# Again parse in only the info tags\n",
    "soup = soup = BeautifulSoup(poisonous_html, \"html.parser\", parse_only=SoupStrainer(attrs='info'))\n",
    "# Call get_mushies handing in the previously built list \n",
    "mushies = get_mushies(soup, mushies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agaricus augustus: Edible good\n",
      "Agaricus campestris: Edible excellent\n",
      "Agaricus Langei: Edible good\n",
      "Agaricus silvicola: Edible good\n",
      "Aleuria aurantia: Edible\n",
      "Amanita rubescens: Edible\n",
      "Armillaria mellea: Edible\n",
      "Auricularia auricula-judae: Edible\n",
      "Boletus appendiculatus: Edible good\n",
      "Boletus badius: Edible good\n",
      "Boletus chrysenteron: Edible\n",
      "Boletus Edulis: Edible excellent\n",
      "Boletus Luridiformis: Edible\n",
      "Boletus luridus: Edible\n",
      "Boletus pruinatus: Edible\n",
      "Calocybe gambosa: Edible\n",
      "Calvatia giantea: Edible good\n",
      "Camarophyllus pratensis: Edible good\n",
      "Cantharellus cibarius: Edible excellent\n",
      "Cantharellus tubaeformis: Edible good\n",
      "Clitocybe geotropa: Edible\n",
      "Clitocybe gibba: Edible\n",
      "Clitocybe odora: Edible\n",
      "Clitopilus prunulus: Edible good\n",
      "Coprinus comatus: Edible\n",
      "Coprinus micaceus: Edible\n",
      "Cuphophyllus pratensis: Edible good\n",
      "Fistulina hepatica: Edible\n",
      "Flammulina velutipes: Edible\n",
      "Grifola frondosa: Edible\n",
      "Handkea excipuliformis: Edible\n",
      "Handkea utriformis: Edible\n",
      "Hydnum repandum: Edible good\n",
      "Hydnum rufescens: Edible good\n",
      "Hygrocybe coccinea: Edible\n",
      "Hygrocybe pratensis: Edible good\n",
      "Hygrocybe punicea: Edible\n",
      "Hygrocybe virginea: Edible good\n",
      "Hygrophorus pratensis: Edible good\n",
      "Kuehneromyces mutabilis: Edible good\n",
      "Laccaria amethystina: Edible\n",
      "Laccaria laccata: Edible\n",
      "Lactarius deliciosus: Edible good\n",
      "Laetiporus sulphureus: Edible good\n",
      "Leccinum scabrum: Edible\n",
      "Leccinum versipelle: Edible good\n",
      "Lepista nuda: Edible excellent\n",
      "Lepista saeva: Edible excellent\n",
      "Lepista sordida: Edible\n",
      "Lycoperdon perlatum: Edible\n",
      "Lycoperdon pyriforme: Edible\n",
      "Macrolepiota mastoidea: Edible good\n",
      "Macrolepiota procera: Edible excellent\n",
      "Macrolepiota rhacodes: Edible good\n",
      "Marasmius oreades: Edible good\n",
      "Morel esculenta: Edible excellent\n",
      "Pleurotus ostreatus: Edible good\n",
      "Pluteus cervinus: Edible\n",
      "Pluteus umbrosus: Edible\n",
      "Polyporus squamosus: Edible\n",
      "Russula cyanoxantha: Edible good\n",
      "Russula ochroleuca: Edible\n",
      "Sparassis crispa: Edible good\n",
      "Suillus bovinus: Edible\n",
      "Suillus grevillei: Edible\n",
      "Suillus luteus: Edible good\n",
      "Vascellum pratense: Edible\n",
      "Agaricus moelleri: Poisonous\n",
      "Amanita muscaria: Poisonous\n",
      "Amanita pantherina: Very poisonous\n",
      "Amanita Phalloides: Deadly poisonous\n",
      "Coprinus atramentarius: Poisonous\n",
      "Gymnopilus junonius: Poisonous\n",
      "Gyromitra esculenta: Deadly poisonous\n",
      "Hebeloma crustuliniforme: Poisonous\n",
      "Hypholoma fasciculare: Poisonous\n",
      "Paxillus involutus: Deadly poisonous\n",
      "Pholiota spectabilis: Poisonous\n",
      "Psilocybe semilanceata: Poisonous\n",
      "Russula emetica: Poisonous\n",
      "Russula nobilis: Poisonous\n"
     ]
    }
   ],
   "source": [
    "for m in mushies:\n",
    "    print(m.spec + ': ' + m.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are a few species of Psychedelic mushroom that were not added to this list, I am going to manually add them as there are only 3. Notice also that Psilocybe semilanceata was included under the type 'Poisonous'. We will edit that to be pyschedelic as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushies.append(Mushroom(\"Psilocybe azurescens\", \"Stamets and Gartz\", \"Psychedelic\", \"Common\"))\n",
    "mushies.append(Mushroom(\"Psilocybe cubensis\", \"Golden Cap\", \"Psychedelic\", \"Common\"))\n",
    "mushies.append(Mushroom(\"Psilocybe cyanescens\", \"Wavy Cap\", \"Psychedelic\", \"Common\"))\n",
    "for m in mushies:\n",
    "    if m.spec == \"Psilocybe semilanceata\":\n",
    "        m.type = \"Psychedelic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agaricus augustus: Edible good\n",
      "Agaricus campestris: Edible excellent\n",
      "Agaricus Langei: Edible good\n",
      "Agaricus silvicola: Edible good\n",
      "Aleuria aurantia: Edible\n",
      "Amanita rubescens: Edible\n",
      "Armillaria mellea: Edible\n",
      "Auricularia auricula-judae: Edible\n",
      "Boletus appendiculatus: Edible good\n",
      "Boletus badius: Edible good\n",
      "Boletus chrysenteron: Edible\n",
      "Boletus Edulis: Edible excellent\n",
      "Boletus Luridiformis: Edible\n",
      "Boletus luridus: Edible\n",
      "Boletus pruinatus: Edible\n",
      "Calocybe gambosa: Edible\n",
      "Calvatia giantea: Edible good\n",
      "Camarophyllus pratensis: Edible good\n",
      "Cantharellus cibarius: Edible excellent\n",
      "Cantharellus tubaeformis: Edible good\n",
      "Clitocybe geotropa: Edible\n",
      "Clitocybe gibba: Edible\n",
      "Clitocybe odora: Edible\n",
      "Clitopilus prunulus: Edible good\n",
      "Coprinus comatus: Edible\n",
      "Coprinus micaceus: Edible\n",
      "Cuphophyllus pratensis: Edible good\n",
      "Fistulina hepatica: Edible\n",
      "Flammulina velutipes: Edible\n",
      "Grifola frondosa: Edible\n",
      "Handkea excipuliformis: Edible\n",
      "Handkea utriformis: Edible\n",
      "Hydnum repandum: Edible good\n",
      "Hydnum rufescens: Edible good\n",
      "Hygrocybe coccinea: Edible\n",
      "Hygrocybe pratensis: Edible good\n",
      "Hygrocybe punicea: Edible\n",
      "Hygrocybe virginea: Edible good\n",
      "Hygrophorus pratensis: Edible good\n",
      "Kuehneromyces mutabilis: Edible good\n",
      "Laccaria amethystina: Edible\n",
      "Laccaria laccata: Edible\n",
      "Lactarius deliciosus: Edible good\n",
      "Laetiporus sulphureus: Edible good\n",
      "Leccinum scabrum: Edible\n",
      "Leccinum versipelle: Edible good\n",
      "Lepista nuda: Edible excellent\n",
      "Lepista saeva: Edible excellent\n",
      "Lepista sordida: Edible\n",
      "Lycoperdon perlatum: Edible\n",
      "Lycoperdon pyriforme: Edible\n",
      "Macrolepiota mastoidea: Edible good\n",
      "Macrolepiota procera: Edible excellent\n",
      "Macrolepiota rhacodes: Edible good\n",
      "Marasmius oreades: Edible good\n",
      "Morel esculenta: Edible excellent\n",
      "Pleurotus ostreatus: Edible good\n",
      "Pluteus cervinus: Edible\n",
      "Pluteus umbrosus: Edible\n",
      "Polyporus squamosus: Edible\n",
      "Russula cyanoxantha: Edible good\n",
      "Russula ochroleuca: Edible\n",
      "Sparassis crispa: Edible good\n",
      "Suillus bovinus: Edible\n",
      "Suillus grevillei: Edible\n",
      "Suillus luteus: Edible good\n",
      "Vascellum pratense: Edible\n",
      "Agaricus moelleri: Poisonous\n",
      "Amanita muscaria: Poisonous\n",
      "Amanita pantherina: Very poisonous\n",
      "Amanita Phalloides: Deadly poisonous\n",
      "Coprinus atramentarius: Poisonous\n",
      "Gymnopilus junonius: Poisonous\n",
      "Gyromitra esculenta: Deadly poisonous\n",
      "Hebeloma crustuliniforme: Poisonous\n",
      "Hypholoma fasciculare: Poisonous\n",
      "Paxillus involutus: Deadly poisonous\n",
      "Pholiota spectabilis: Poisonous\n",
      "Psilocybe semilanceata: Psychedelic\n",
      "Russula emetica: Poisonous\n",
      "Russula nobilis: Poisonous\n",
      "Psilocybe azurescens: Psychedelic\n",
      "Psilocybe cubensis: Psychedelic\n",
      "Psilocybe cyanescens: Psychedelic\n",
      "84 Mushrooms foraged from the internet!\n"
     ]
    }
   ],
   "source": [
    "for m in mushies:\n",
    "    print(m.spec + ': ' + m.type)\n",
    "print(str(len(mushies)) + ' Mushrooms foraged from the internet!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting URLS of Mushroom Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a non-exhaustive list of mushroom species, some of them edible, some of them poisonous, and a few psyhcedelic. We have their common name, and frequency as well. We could really build out this dataset and include more information to genuinely create a mushroom equivalent of a Pokédex, but I will save that project for another time. The main goal of this is to get hands on with the FastAi package and train a deep learning model. \n",
    "\n",
    "The next step is to collect URLs of images of each species using google images. Using google images is not perfect, we can not be so sure of these mushrooms being correctly labelled, for that we would need access to a better source - some kind of visual encyclopedia of foragable mushrooms, such a thing does not exist..yet. Furthermore, my first attempts at building a deep learning model to predict all 84 species was only 50% accurate, so this time I'm attempting the simpler task of classifying non-psychedelic and psychedelic species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, time\n",
    "from fastai.vision import *\n",
    "\n",
    "# String -> URL\n",
    "# Create a google image search URL (reusable images only) given a query string\n",
    "def make_google_image_URL(query):\n",
    "    query = '\"' + query.replace(' ', '+') + '\"'\n",
    "    return \"https://www.google.com/\"\\\n",
    "           \"search?q=\"+query+\"+-cartoon+-drawing+-diagram+-art+-text&tbm=isch&safe=off\" # No cartoon images\n",
    "\n",
    "\n",
    "# String Integer -> ListOfURLs\n",
    "# Given a google image search query and a number of images, return a list of URLs that is num_imgs long\n",
    "# Original function idea from here: https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d\n",
    "# The below function is a heavily modified version of the original:\n",
    "def fetch_image_urls(species, query:str, num_imgs:int, sleep_between_interactions:int=1):\n",
    "        \n",
    "    # Fn to scroll to end of google images results page (loads more images)\n",
    "    def scroll_to_end(wd):\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_between_interactions)    \n",
    "        \n",
    "    # Instantiate Selenium WebDriver and load query page\n",
    "    wd = webdriver.Chrome(executable_path=path)\n",
    "    wd.get(query)\n",
    "      \n",
    "    # Go through all img tags and add source to a list of image URLs \n",
    "    # until desired number of URLs have been gathered\n",
    "    \n",
    "    image_urls = set() # Avoids retreival of duplicate URLs\n",
    "    image_count = 0\n",
    "    results_start = 0\n",
    "    attempts = 0 # Allows for breaking of loop if can't find desired number of links\n",
    "    while image_count < num_imgs:\n",
    "        \n",
    "        html = wd.page_source\n",
    "        scroll_to_end(wd)\n",
    "            \n",
    "        # get html and all img tags using BeautifulSoup after scrolling\n",
    "        images = BeautifulSoup(html, \"html.parser\", parse_only=SoupStrainer('img'))\n",
    "        \n",
    "        number_results = len(images)\n",
    "        \n",
    "        if number_results == 0:\n",
    "            print(\"No images retrieved. Check your query or html selectors.\")\n",
    "            break\n",
    "        \n",
    "        for img in images:\n",
    "            # extract image urls\n",
    "            if img.has_attr('src'):\n",
    "                image_urls.add(img['src'])   \n",
    "\n",
    "        image_count = len(image_urls)\n",
    "\n",
    "        if image_count >= num_imgs:\n",
    "            print(f\"Found {image_count} image links for {species}.\")\n",
    "            break\n",
    "        try:\n",
    "            wd.find_element_by_xpath('/html/body/div[2]/c-wiz/div[3]/div[1]/div/div/div/div/div[5]/input').click()\n",
    "        except:\n",
    "            if attempts > 1000:\n",
    "                print(f\"Could only find {image_count} image links for {species}.\")\n",
    "                break\n",
    "            else:\n",
    "                attempts += 1\n",
    "\n",
    "        # move the result startpoint further down\n",
    "        results_start = number_results\n",
    "    \n",
    "    wd.quit()\n",
    "    \n",
    "    return list(image_urls)\n",
    "\n",
    "\n",
    "# ListOfObjects Attribute -> Images in folder: object/attribute/images\n",
    "# Function to collect google images based on an object attribute from a list of objects\n",
    "# In this case we have a list of mushroom objects\n",
    "# If using this in another setting, you may need to change paths to folders, files, chromedriver etc.\n",
    "\n",
    "def get_URL_CSVs(list_of_objs, attr, num_imgs):\n",
    "    \n",
    "    # Make a folder to store the images (if it doesn't already exist)\n",
    "    # Will create a dir like this: ./nameofobjectclass\n",
    "    folder = type(list_of_objs[0]).__name__.lower()\n",
    "    if not os.path.isdir(folder+'-images'):\n",
    "        os.makedirs(folder+'-images', exist_ok=True)\n",
    "        out_dir = folder+'-images'\n",
    "    else:\n",
    "        out_dir = folder+'-images'\n",
    "    \n",
    "    for o in list_of_objs:\n",
    "        \n",
    "        # Create folder labels and csv filename\n",
    "        # Will create nameofobjectclass/attribute/attribute.csv\n",
    "        query = str(getattr(o,attr))\n",
    "        n_dir = query.replace(' ', '-').lower()\n",
    "        new_dir = out_dir + '/' + n_dir\n",
    "        filename = n_dir + '.csv'\n",
    "        \n",
    "        # If file containing URLs already exists go to next object: \n",
    "        if os.path.isfile(new_dir+'/'+filename):\n",
    "            continue\n",
    "        else:\n",
    "            # Get list of URLs\n",
    "            query_url = make_google_image_URL(query)\n",
    "            URLs = fetch_image_urls(query, query_url, num_imgs)\n",
    "                        \n",
    "            # Write list of URLs to CSV in a new folder labelled by the attribute \n",
    "            os.makedirs(new_dir, exist_ok=True)\n",
    "            with open(new_dir+'/'+filename, 'w') as f:\n",
    "                URLs = map(lambda x:x+'\\n', URLs)\n",
    "                f.writelines(URLs)\n",
    "        \n",
    "get_URL_CSVs(mushies, 'spec', 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have scraped our mushroom image URLs from google images and stored them in CSV files that are in separate folders labelled according to their species. Each species folder is contained in a folder named after the Mushroom class ('mushroom-images'). Now to simplify our folder and class structure, I will combine all the CSVs from non-psilocybe species into one CSV in one folder called 'non-psychedelic'. We will upload the entire 'mushroom-images' folder to paperspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['non-psychedelic', 'psilocybe-semilanceata', 'psilocybe-azurescens', 'psilocybe-cubensis', 'psilocybe-cyanescens']\n",
      "['agaricus-augustus', 'agaricus-campestris', 'agaricus-langei', 'agaricus-silvicola', 'aleuria-aurantia', 'amanita-rubescens', 'armillaria-mellea', 'auricularia-auricula-judae', 'boletus-appendiculatus', 'boletus-badius', 'boletus-chrysenteron', 'boletus-edulis', 'boletus-luridiformis', 'boletus-luridus', 'boletus-pruinatus', 'calocybe-gambosa', 'calvatia-giantea', 'camarophyllus-pratensis', 'cantharellus-cibarius', 'cantharellus-tubaeformis', 'clitocybe-geotropa', 'clitocybe-gibba', 'clitocybe-odora', 'clitopilus-prunulus', 'coprinus-comatus', 'coprinus-micaceus', 'cuphophyllus-pratensis', 'fistulina-hepatica', 'flammulina-velutipes', 'grifola-frondosa', 'handkea-excipuliformis', 'handkea-utriformis', 'hydnum-repandum', 'hydnum-rufescens', 'hygrocybe-coccinea', 'hygrocybe-pratensis', 'hygrocybe-punicea', 'hygrocybe-virginea', 'hygrophorus-pratensis', 'kuehneromyces-mutabilis', 'laccaria-amethystina', 'laccaria-laccata', 'lactarius-deliciosus', 'laetiporus-sulphureus', 'leccinum-scabrum', 'leccinum-versipelle', 'lepista-nuda', 'lepista-saeva', 'lepista-sordida', 'lycoperdon-perlatum', 'lycoperdon-pyriforme', 'macrolepiota-mastoidea', 'macrolepiota-procera', 'macrolepiota-rhacodes', 'marasmius-oreades', 'morel-esculenta', 'pleurotus-ostreatus', 'pluteus-cervinus', 'pluteus-umbrosus', 'polyporus-squamosus', 'russula-cyanoxantha', 'russula-ochroleuca', 'sparassis-crispa', 'suillus-bovinus', 'suillus-grevillei', 'suillus-luteus', 'vascellum-pratense', 'agaricus-moelleri', 'amanita-muscaria', 'amanita-pantherina', 'amanita-phalloides', 'coprinus-atramentarius', 'gymnopilus-junonius', 'gyromitra-esculenta', 'hebeloma-crustuliniforme', 'hypholoma-fasciculare', 'paxillus-involutus', 'pholiota-spectabilis', 'psilocybe-semilanceata', 'russula-emetica', 'russula-nobilis', 'psilocybe-azurescens', 'psilocybe-cubensis', 'psilocybe-cyanescens']\n"
     ]
    }
   ],
   "source": [
    "# Generate the 5 classes, and a list of all directories in the mushroom-images folder\n",
    "dirs = []\n",
    "classes = ['non-psychedelic']\n",
    "for m in mushies:\n",
    "    if m.spec.startswith('Psilocybe'):\n",
    "        classes.append(m.spec.replace(' ', '-').lower())\n",
    "    dirs.append(m.spec.replace(' ', '-').lower())\n",
    "print(classes)\n",
    "print(dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only continue if you want to simplify the classes from 84 different species into 5: non-psychedelic and the 4 psilocybe species. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/Users/alexmars/Documents/CS/Personal-Projects/FastAI/Week1/mushroom-images-v2/non-psychedelic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f87e8eeaa360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnon_psyche\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'mushroom-images-v2'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'non-psychedelic'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_psyche\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# Copy all CSVs from the non-psilocybe species into the non-psychedelic dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Copy the psilocybe dirs into the mushroom-images-v2 folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/Users/alexmars/Documents/CS/Personal-Projects/FastAI/Week1/mushroom-images-v2/non-psychedelic'"
     ]
    }
   ],
   "source": [
    "# Create a new mushroom-images dir with a 'non-psychedelic' sub-dir\n",
    "main = os.getcwd()\n",
    "non_psyche = main + '/' + 'mushroom-images-v2' + '/' + 'non-psychedelic'\n",
    "os.makedirs(non_psyche)\n",
    "# Copy all CSVs from the non-psilocybe species into the non-psychedelic dir\n",
    "# Copy the psilocybe dirs into the mushroom-images-v2 folder\n",
    "for d in dirs:\n",
    "    if not d.startswith('psilocybe'):\n",
    "        shutil.copy(main + '/mushroom-images' + '/' + d + '/' + d + '.csv', non_psyche)\n",
    "    else:\n",
    "        shutil.copytree(main + '/mushroom-images' + '/' + d, main + '/' + 'mushroom-images-v2' + '/' + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the CSVs in the non-psychedelic dir into one CSV\n",
    "newfile = open(non_psyche + '/' + 'non-psychedelic.csv', 'a')\n",
    "for d in dirs:\n",
    "    if not d.startswith('psilocybe'):\n",
    "        with open(non_psyche+'/' + d + '.csv') as f:\n",
    "            for line in f:\n",
    "                newfile.write(line)\n",
    "        os.remove(non_psyche+'/' + d + '.csv')\n",
    "newfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have only 5 classes: non-psychedelic, and the 4 psilocybe species organised into 5 individual CSVs in 5 directories, all within the mushroom-images-v2 folder. We will upload this entire folder to paperspace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continued in 'week1-cnn' which requires a GPU linked notebook instance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
